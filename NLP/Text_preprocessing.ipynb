{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Lower Case\n",
    "\n",
    "In this step, we convert the given sentence to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the sky is not the limit'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"The Sky is not the limit\"\n",
    "lowered_sent =sentence.lower()\n",
    "lowered_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Remove Stop Words\n",
    "\n",
    "In this step, we remove common stop words from the sentence to focus on the important words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\akaash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\akaash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\akaash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\akaash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\akaash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\akaash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Akaash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_stops = stopwords.words('english')\n",
    "en_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I am not able to come up with a way to solve this problem that is why I am delaying this project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I able come way solve problem I delaying project'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_no_stopwords = ' '.join([word for word in sentence.split() if word not in en_stops])\n",
    "sentence_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stops.remove('up')\n",
    "en_stops.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence_no_stopwords = ' '.join([word for word in sentence.split() if word not in en_stops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I not able come up way solve problem I delaying project'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sentence_no_stopwords "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Regular Expression\n",
    "\n",
    "In this step, we will use regular expressions to further clean and process the text data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 'not' in the sentence at position 5\n",
      "I am definitely able to come up with a way to solve this problem that is why I am delaying this project\n"
     ]
    }
   ],
   "source": [
    "# Example of re.search\n",
    "pattern = r'\\bnot\\b'\n",
    "match = re.search(pattern, sentence)\n",
    "if match:\n",
    "    print(f\"Found '{match.group()}' in the sentence at position {match.start()}\")\n",
    "\n",
    "# Example of re.sub\n",
    "pattern = r'\\bnot\\b'\n",
    "replacement = 'definitely'\n",
    "new_sentence = re.sub(pattern, replacement, sentence)\n",
    "print(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts with 'Alice': Alice did a fantastic job!\n",
      "Ends with 'commendable.': Frank's dedication is commendable.\n",
      "Contains 'Alice' or 'Bob': Alice did a fantastic job!\n",
      "Contains 'Alice' or 'Bob': Bob's work was exceptional.\n",
      "Reviews without punctuations: ['Alice did a fantastic job', 'Bobs work was exceptional', 'Charlie is a great team player', 'Davids contribution was invaluable', 'Eves performance was outstanding', 'Franks dedication is commendable']\n"
     ]
    }
   ],
   "source": [
    "# List of reviews\n",
    "reviews = [\n",
    "    \"Alice did a fantastic job!\",\n",
    "    \"Bob's work was exceptional.\",\n",
    "    \"Charlie is a great team player.\",\n",
    "    \"David's contribution was invaluable.\",\n",
    "    \"Eve's performance was outstanding.\",\n",
    "    \"Frank's dedication is commendable.\"\n",
    "]\n",
    "\n",
    "# Demonstrating the use of ^ (start of string)\n",
    "for review in reviews:\n",
    "    if re.search(r'^Alice', review):\n",
    "        print(f\"Starts with 'Alice': {review}\")\n",
    "\n",
    "# Demonstrating the use of $ (end of string)\n",
    "for review in reviews:\n",
    "    if re.search(r'commendable\\.$', review):\n",
    "        print(f\"Ends with 'commendable.': {review}\")\n",
    "\n",
    "# Demonstrating the use of | (or)\n",
    "for review in reviews:\n",
    "    if re.search(r'Alice|Bob', review):\n",
    "        print(f\"Contains 'Alice' or 'Bob': {review}\")\n",
    "\n",
    "# Removing punctuations from reviews\n",
    "reviews_no_punctuations = [re.sub(r'[^\\w\\s]', '', review) for review in reviews]\n",
    "print(\"Reviews without punctuations:\", reviews_no_punctuations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Tokenization\n",
    "\n",
    "In this step, we will tokenize the sentences into individual words. Tokenization is the process of splitting text into smaller pieces, such as words or phrases. This is a crucial step in text processing as it allows us to analyze the text at a granular level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\akaash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: click in c:\\users\\akaash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\akaash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\akaash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\akaash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\akaash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Akaash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Akaash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['Hello there!', 'How are you doing today?', 'This is a simple sentence tokenizer and word tokenizer demo.']\n",
      "Words: ['Hello', 'there', '!', 'How', 'are', 'you', 'doing', 'today', '?', 'This', 'is', 'a', 'simple', 'sentence', 'tokenizer', 'and', 'word', 'tokenizer', 'demo', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Example string\n",
    "example_string = \"Hello there! How are you doing today? This is a simple sentence tokenizer and word tokenizer demo.\"\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(example_string)\n",
    "print(\"Sentences:\", sentences)\n",
    "\n",
    "# Word Tokenization\n",
    "words = word_tokenize(example_string)\n",
    "print(\"Words:\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Stemming\n",
    "\n",
    "In this step, we will apply stemming to reduce words to their root form. Stemming helps in reducing inflected or derived words to their base form, which is useful in text processing as it reduces the dimensionality of the data and improves text analysis tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: connect, Stemmed: connect\n",
      "Original: connected, Stemmed: connect\n",
      "Original: connecting, Stemmed: connect\n",
      "Original: connection, Stemmed: connect\n",
      "Original: connections, Stemmed: connect\n",
      "Original: learn, Stemmed: learn\n",
      "Original: learning, Stemmed: learn\n",
      "Original: learned, Stemmed: learn\n",
      "Original: learner, Stemmed: learner\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize the stemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# Example tokens\n",
    "tokens = [\"connect\", \"connected\", \"connecting\", \"connection\", \"connections\", \"learn\", \"learning\", \"learned\", \"learner\"]\n",
    "\n",
    "# Apply stemming\n",
    "stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
    "\n",
    "# Display the results\n",
    "for token, stemmed in zip(tokens, stemmed_tokens):\n",
    "    print(f\"Original: {token}, Stemmed: {stemmed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 6: Lemmatization\n",
    "\n",
    "In this step, we will apply lemmatization to reduce words to their base or dictionary form. Lemmatization is similar to stemming but it brings context to the words. It links words with similar meanings to one word. This is useful in text processing as it helps in understanding the context and meaning of the words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Akaash\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: connect, Lemmatized: connect\n",
      "Original: connected, Lemmatized: connected\n",
      "Original: connecting, Lemmatized: connecting\n",
      "Original: connection, Lemmatized: connection\n",
      "Original: connections, Lemmatized: connection\n",
      "Original: learn, Lemmatized: learn\n",
      "Original: learning, Lemmatized: learning\n",
      "Original: learned, Lemmatized: learned\n",
      "Original: learner, Lemmatized: learner\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example tokens\n",
    "tokens = [\"connect\", \"connected\", \"connecting\", \"connection\", \"connections\", \"learn\", \"learning\", \"learned\", \"learner\"]\n",
    "\n",
    "# Apply lemmatization\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Display the results\n",
    "for token, lemmatized in zip(tokens, lemmatized_tokens):\n",
    "    print(f\"Original: {token}, Lemmatized: {lemmatized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### Stemming vs Lemmatization\n",
    "\n",
    "**Stemming:**\n",
    "- Reduces words to their root form by removing suffixes.\n",
    "- Often results in non-dictionary words.\n",
    "- Example: \"connected\" -> \"connect\", \"connecting\" -> \"connect\".\n",
    "\n",
    "**Lemmatization:**\n",
    "- Reduces words to their base or dictionary form.\n",
    "- Considers the context and meaning of the words.\n",
    "- Example: \"connected\" -> \"connected\", \"better\" -> \"good\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: [('This', 'is'), ('is', 'a'), ('a', 'simple'), ('simple', 'example'), ('example', 'to'), ('to', 'demonstrate'), ('demonstrate', 'the'), ('the', 'use'), ('use', 'of'), ('of', 'ngrams'), ('ngrams', 'in'), ('in', 'nltk')]\n",
      "Trigrams: [('This', 'is', 'a'), ('is', 'a', 'simple'), ('a', 'simple', 'example'), ('simple', 'example', 'to'), ('example', 'to', 'demonstrate'), ('to', 'demonstrate', 'the'), ('demonstrate', 'the', 'use'), ('the', 'use', 'of'), ('use', 'of', 'ngrams'), ('of', 'ngrams', 'in'), ('ngrams', 'in', 'nltk')]\n",
      "Bigram Frequencies: Counter({('This', 'is'): 1, ('is', 'a'): 1, ('a', 'simple'): 1, ('simple', 'example'): 1, ('example', 'to'): 1, ('to', 'demonstrate'): 1, ('demonstrate', 'the'): 1, ('the', 'use'): 1, ('use', 'of'): 1, ('of', 'ngrams'): 1, ('ngrams', 'in'): 1, ('in', 'nltk'): 1})\n",
      "Trigram Frequencies: Counter({('This', 'is', 'a'): 1, ('is', 'a', 'simple'): 1, ('a', 'simple', 'example'): 1, ('simple', 'example', 'to'): 1, ('example', 'to', 'demonstrate'): 1, ('to', 'demonstrate', 'the'): 1, ('demonstrate', 'the', 'use'): 1, ('the', 'use', 'of'): 1, ('use', 'of', 'ngrams'): 1, ('of', 'ngrams', 'in'): 1, ('ngrams', 'in', 'nltk'): 1})\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Example string\n",
    "dummy_data = \"This is a simple example to demonstrate the use of ngrams in nltk\"\n",
    "\n",
    "# Tokenize the string into words\n",
    "tokens = word_tokenize(dummy_data)\n",
    "\n",
    "# Generate bigrams (2-grams)\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "print(\"Bigrams:\", bigrams)\n",
    "\n",
    "# Generate trigrams (3-grams)\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "print(\"Trigrams:\", trigrams)\n",
    "\n",
    "# Count the frequency of bigrams\n",
    "bigram_freq = Counter(bigrams)\n",
    "print(\"Bigram Frequencies:\", bigram_freq)\n",
    "\n",
    "# Count the frequency of trigrams\n",
    "trigram_freq = Counter(trigrams)\n",
    "print(\"Trigram Frequencies:\", trigram_freq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
